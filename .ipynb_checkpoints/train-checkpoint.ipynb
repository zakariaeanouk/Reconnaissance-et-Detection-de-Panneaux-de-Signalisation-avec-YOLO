{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a665f061-a7a3-4863-bf5c-f02a7730e45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Chemins vers les fichiers\n",
    "train_csv = \"GTSRB/Train.csv\"\n",
    "train_images_dir = \"GTSRB/Train\"\n",
    "\n",
    "# Charger les annotations CSV\n",
    "annotations = pd.read_csv(train_csv)\n",
    "\n",
    "# Fonction pour convertir et créer les fichiers .txt\n",
    "def convert_to_yolo(annotations, images_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for _, row in annotations.iterrows():\n",
    "        image_width, image_height = row['Width'], row['Height']\n",
    "        x_min, y_min, x_max, y_max = row['Roi.X1'], row['Roi.Y1'], row['Roi.X2'], row['Roi.Y2']\n",
    "        class_id = row['ClassId']\n",
    "        \n",
    "        # Calcul des coordonnées normalisées\n",
    "        x_center = (x_min + x_max) / 2 / image_width\n",
    "        y_center = (y_min + y_max) / 2 / image_height\n",
    "        width = (x_max - x_min) / image_width\n",
    "        height = (y_max - y_min) / image_height\n",
    "        \n",
    "        # Créer un fichier .txt pour chaque image\n",
    "        image_path = row['Path']\n",
    "        filename = os.path.splitext(os.path.basename(image_path))[0] + \".txt\"\n",
    "        with open(os.path.join(output_dir, filename), 'w') as f:\n",
    "            f.write(f\"{class_id} {x_center} {y_center} {width} {height}\\n\")\n",
    "\n",
    "# Convertir les annotations d'entraînement\n",
    "convert_to_yolo(annotations, train_images_dir, \"GTSRB/train/labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff4004f8-acff-4fab-8c59-cddf02d98896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Chemins vers les fichiers\n",
    "train_csv = \"GTSRB/Test.csv\"\n",
    "train_images_dir = \"GTSRB/Test\"\n",
    "\n",
    "# Charger les annotations CSV\n",
    "annotations = pd.read_csv(train_csv)\n",
    "\n",
    "# Fonction pour convertir et créer les fichiers .txt\n",
    "def convert_to_yolo(annotations, images_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for _, row in annotations.iterrows():\n",
    "        image_width, image_height = row['Width'], row['Height']\n",
    "        x_min, y_min, x_max, y_max = row['Roi.X1'], row['Roi.Y1'], row['Roi.X2'], row['Roi.Y2']\n",
    "        class_id = row['ClassId']\n",
    "        \n",
    "        # Calcul des coordonnées normalisées\n",
    "        x_center = (x_min + x_max) / 2 / image_width\n",
    "        y_center = (y_min + y_max) / 2 / image_height\n",
    "        width = (x_max - x_min) / image_width\n",
    "        height = (y_max - y_min) / image_height\n",
    "        \n",
    "        # Créer un fichier .txt pour chaque image\n",
    "        image_path = row['Path']\n",
    "        filename = os.path.splitext(os.path.basename(image_path))[0] + \".txt\"\n",
    "        with open(os.path.join(output_dir, filename), 'w') as f:\n",
    "            f.write(f\"{class_id} {x_center} {y_center} {width} {height}\\n\")\n",
    "\n",
    "# Convertir les annotations d'entraînement\n",
    "convert_to_yolo(annotations, train_images_dir, \"GTSRB/Test/labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d8857ec-8297-468c-a07c-7edb21fe61f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#déplacer les images dans un sous dossier images\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Dossiers source\n",
    "datasets = [\"Train\"]\n",
    "\n",
    "for dataset in datasets:\n",
    "    # Créer le dossier images si nécessaire\n",
    "    images_dir = os.path.join(\"GTSRB\", dataset, \"images\")\n",
    "    os.makedirs(images_dir, exist_ok=True)\n",
    "    \n",
    "    # Parcourir les sous-dossiers correspondant aux classes\n",
    "    class_dirs = [str(i) for i in range(43)]  # Classes de 0 à 42\n",
    "    for class_dir in class_dirs:\n",
    "        class_path = os.path.join(\"GTSRB\", dataset, class_dir)\n",
    "        if os.path.exists(class_path):  # Vérifier que le sous-dossier existe\n",
    "            # Déplacer toutes les images de ce sous-dossier\n",
    "            for file in os.listdir(class_path):\n",
    "                if file.endswith(\".png\"):  # Vérifie les fichiers image\n",
    "                    shutil.move(\n",
    "                        os.path.join(class_path, file),\n",
    "                        os.path.join(images_dir, file)\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b8744fd-e783-4f3e-acd1-fe8770090996",
   "metadata": {},
   "outputs": [],
   "source": [
    "#déplacer les images dans un sous dossier images pour\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Dossiers source\n",
    "datasets = [\"Test\", \"Meta\"]\n",
    "\n",
    "for dataset in datasets:\n",
    "    # Créer le dossier images si nécessaire\n",
    "    images_dir = os.path.join(\"GTSRB\", dataset, \"images\")\n",
    "    os.makedirs(images_dir, exist_ok=True)\n",
    "    \n",
    "    # Déplacer toutes les images dans le dossier images/\n",
    "    for file in os.listdir(os.path.join(\"GTSRB\", dataset)):\n",
    "        if file.endswith(\".png\"):  # Vérifie les fichiers image\n",
    "            shutil.move(\n",
    "                os.path.join(\"GTSRB\", dataset, file),\n",
    "                os.path.join(images_dir, file)\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ccde3ec-08ca-42bb-ba0f-d67ab46adb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les chemins dans les fichiers CSV ont été mis à jour avec succès.\n"
     ]
    }
   ],
   "source": [
    "#modifier le path dans les dossiers csv\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Fichiers CSV et dossiers d'images correspondants\n",
    "csv_files = {\n",
    "    \"Train.csv\": \"GTSRB/Train/images\",\n",
    "    \"Test.csv\": \"GTSRB/Test/images\",\n",
    "    \"Meta.csv\": \"GTSRB/Meta/images\"\n",
    "}\n",
    "\n",
    "for csv_file, images_dir in csv_files.items():\n",
    "    # Charger le fichier CSV\n",
    "    csv_path = os.path.join(\"GTSRB\", csv_file)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Mettre à jour la colonne 'path' pour refléter la nouvelle structure\n",
    "    df['Path'] = df['Path'].apply(lambda x: os.path.join(images_dir, os.path.basename(x)))\n",
    "    \n",
    "    # Sauvegarder les modifications dans le fichier CSV\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(\"Les chemins dans les fichiers CSV ont été mis à jour avec succès.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7800e474-59db-46e5-ba08-954735d2eac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le dataset a été divisé en Train et Validation avec succès.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Chemins vers les dossiers d'entrée et de sortie\n",
    "images_dir = \"/Users/zakariaeanouk/Desktop/projet ML/GTSRB/Train/images\"\n",
    "labels_dir = \"/Users/zakariaeanouk/Desktop/projet ML/GTSRB/Train/labels\"\n",
    "output_dir = \"/Users/zakariaeanouk/Desktop/projet ML/GTSRB\"\n",
    "\n",
    "# Dossiers de sortie pour le nouveau Train et Validation\n",
    "new_train_images_dir = os.path.join(output_dir, \"New_Train/images\")\n",
    "new_train_labels_dir = os.path.join(output_dir, \"New_Train/labels\")\n",
    "val_images_dir = os.path.join(output_dir, \"Validation/images\")\n",
    "val_labels_dir = os.path.join(output_dir, \"Validation/labels\")\n",
    "\n",
    "# Créer les dossiers pour Train et Validation\n",
    "os.makedirs(new_train_images_dir, exist_ok=True)\n",
    "os.makedirs(new_train_labels_dir, exist_ok=True)\n",
    "os.makedirs(val_images_dir, exist_ok=True)\n",
    "os.makedirs(val_labels_dir, exist_ok=True)\n",
    "\n",
    "# Liste de toutes les images et leurs labels correspondants\n",
    "all_images = [f for f in os.listdir(images_dir) if f.endswith(\".png\")]\n",
    "all_labels = [f.replace(\".png\", \".txt\") for f in all_images]\n",
    "\n",
    "# Diviser les données : 80% pour Train et 20% pour Validation\n",
    "train_images, val_images = train_test_split(all_images, test_size=0.2, random_state=42)\n",
    "\n",
    "# Copier les fichiers dans les dossiers correspondants\n",
    "for image in train_images:\n",
    "    shutil.copy(os.path.join(images_dir, image), os.path.join(new_train_images_dir, image))\n",
    "    shutil.copy(os.path.join(labels_dir, image.replace(\".png\", \".txt\")), os.path.join(new_train_labels_dir, image.replace(\".png\", \".txt\")))\n",
    "\n",
    "for image in val_images:\n",
    "    shutil.copy(os.path.join(images_dir, image), os.path.join(val_images_dir, image))\n",
    "    shutil.copy(os.path.join(labels_dir, image.replace(\".png\", \".txt\")), os.path.join(val_labels_dir, image.replace(\".png\", \".txt\")))\n",
    "\n",
    "print(\"Le dataset a été divisé en Train et Validation avec succès.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "673bc96a-96cf-488d-b481-fd3948975510",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/zakariaeanouk/Downloads/p10.png: 640x320 1 Speed_Limit_30, 1 No_overtaking, 81.7ms\n",
      "Speed: 1.9ms preprocess, 81.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 320)\n",
      "Results saved to \u001b[1mruns/detect/predict75\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'Speed_Limit_20', 1: 'Speed_Limit_30', 2: 'Speed_Limit_50', 3: 'Speed_Limit_60', 4: 'Speed_Limit_70', 5: 'Speed_Limit_80', 6: 'End_of_speed_limit', 7: 'Speed_Limit_100', 8: 'Speed_Limit_120', 9: 'No_overtaking', 10: 'No_overtaking_by_trucks', 11: 'Intersection_with_a_minor_road', 12: 'Priority_road', 13: 'Give_way', 14: 'Stop', 15: 'Road_closed_to_all_vehicles', 16: 'No_trucks', 17: 'No_entry', 18: 'Other_dangers', 19: 'Curve_to_the_left', 20: 'Curve_to_the_right', 21: 'Double_curve_first_to_left', 22: 'Uneven_road', 23: 'Slippery_road', 24: 'Road_narrows_on_the_right', 25: 'Roadworks', 26: 'Traffic_lights', 27: 'Pedestrian_crossing_ahead', 28: 'Children', 29: 'Bicycle crossing', 30: 'Snow', 31: 'Deer_crossing', 32: 'End_of_all_restrictions_and_prohibitions', 33: 'Turn_right_only', 34: 'Turn_left_only', 35: 'Go_straight_only', 36: 'Proceed_straight_or_turn_right', 37: 'Proceed_straight_or_turn_left', 38: 'Keep_right', 39: 'Keep_left', 40: 'Roundabout', 41: 'End_overtaking_prohibition', 42: 'End_overtaking_prohibition_for_trucks'}\n",
       " obb: None\n",
       " orig_img: array([[[28, 54, 46],\n",
       "         [28, 54, 46],\n",
       "         [27, 52, 44],\n",
       "         ...,\n",
       "         [19, 41, 32],\n",
       "         [15, 39, 28],\n",
       "         [15, 40, 29]],\n",
       " \n",
       "        [[31, 58, 49],\n",
       "         [28, 55, 46],\n",
       "         [28, 54, 45],\n",
       "         ...,\n",
       "         [17, 40, 32],\n",
       "         [12, 35, 26],\n",
       "         [10, 34, 24]],\n",
       " \n",
       "        [[35, 63, 55],\n",
       "         [31, 59, 50],\n",
       "         [28, 55, 46],\n",
       "         ...,\n",
       "         [18, 41, 33],\n",
       "         [12, 36, 27],\n",
       "         [11, 33, 24]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[17, 38, 29],\n",
       "         [21, 42, 32],\n",
       "         [26, 46, 37],\n",
       "         ...,\n",
       "         [11, 80, 55],\n",
       "         [ 3, 74, 49],\n",
       "         [ 0, 71, 46]],\n",
       " \n",
       "        [[19, 47, 35],\n",
       "         [25, 53, 41],\n",
       "         [28, 53, 41],\n",
       "         ...,\n",
       "         [ 7, 75, 49],\n",
       "         [ 2, 72, 45],\n",
       "         [ 0, 69, 42]],\n",
       " \n",
       "        [[24, 54, 41],\n",
       "         [31, 61, 48],\n",
       "         [34, 62, 49],\n",
       "         ...,\n",
       "         [ 4, 72, 44],\n",
       "         [ 3, 72, 45],\n",
       "         [ 1, 72, 44]]], dtype=uint8)\n",
       " orig_shape: (404, 196)\n",
       " path: '/Users/zakariaeanouk/Downloads/p10.png'\n",
       " probs: None\n",
       " save_dir: 'runs/detect/predict75'\n",
       " speed: {'preprocess': 1.8680095672607422, 'inference': 81.65884017944336, 'postprocess': 0.7688999176025391}]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Créer un modèle YOLOv8 à partir d'un modèle pré-entraîné (par exemple, YOLOv8 nano)\n",
    "model = YOLO(\"/Users/zakariaeanouk/Desktop/projet ML/best.pt\")  # Vous pouvez aussi essayer d'autres versions comme yolov8s.pt, yolov8m.pt, etc.\n",
    "\n",
    "# Entraînement\n",
    "results = model.predict(source=\"/Users/zakariaeanouk/Downloads/p10.png\", save=True)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a29af60-cdc4-4cdf-9313-29ed7aef31e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Charger l'image\n",
    "image = cv2.imread(\"/Users/zakariaeanouk/Downloads/archive (1)/ts/ts/00024.jpg\")\n",
    "hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Détecter les zones rouges\n",
    "lower_red = np.array([0, 50, 50])\n",
    "upper_red = np.array([10, 255, 255])\n",
    "mask_red = cv2.inRange(hsv, lower_red, upper_red)\n",
    "\n",
    "# Appliquer la détection de contours\n",
    "contours, _ = cv2.findContours(mask_red, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "for cnt in contours:\n",
    "    x, y, w, h = cv2.boundingRect(cnt)\n",
    "    cropped = image[y:y+h, x:x+w]\n",
    "    cv2.imwrite(f\"cropped_{x}_{y}.jpg\", cropped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4c94844-498f-4d49-b458-33ef1c58223b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chemins mis à jour et enregistrés dans /Users/zakariaeanouk/Downloads/archive (1)/test_updated.txt\n"
     ]
    }
   ],
   "source": [
    "# Chemin du fichier train.txt\n",
    "input_file = \"/Users/zakariaeanouk/Downloads/archive (1)/test.txt\"\n",
    "output_file = \"/Users/zakariaeanouk/Downloads/archive (1)/test_updated.txt\"  # Nouveau fichier avec les chemins corrigés\n",
    "new_image_dir = \"/Users/zakariaeanouk/Downloads/archive (1)/ts/ts/\"  # Nouveau dossier des images\n",
    "\n",
    "# Lire et modifier les chemins\n",
    "with open(input_file, \"r\") as infile, open(output_file, \"w\") as outfile:\n",
    "    for line in infile:\n",
    "        # Obtenir le nom du fichier (sans l'ancien chemin)\n",
    "        image_name = line.strip().split(\"/\")[-1]\n",
    "        # Construire le nouveau chemin\n",
    "        new_path = f\"{new_image_dir}{image_name}\"\n",
    "        # Écrire le nouveau chemin dans le fichier de sortie\n",
    "        outfile.write(new_path + \"\\n\")\n",
    "\n",
    "print(f\"Chemins mis à jour et enregistrés dans {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f54f2f78-c34d-46d4-af42-8f142da9a9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les images ont été copiées dans le dossier 'train/images'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Chemin vers le fichier train.txt\n",
    "train_txt_path = \"/Users/zakariaeanouk/Downloads/archive (1)/test_updated.txt\"\n",
    "\n",
    "# Chemin du dossier où les images seront copiées\n",
    "output_dir = \"/Users/zakariaeanouk/Downloads/archive (1)/Test/images\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Créer le dossier s'il n'existe pas\n",
    "\n",
    "# Lire les chemins des images depuis train.txt\n",
    "with open(train_txt_path, \"r\") as file:\n",
    "    image_paths = file.readlines()\n",
    "\n",
    "# Supprimer les espaces ou caractères inutiles\n",
    "image_paths = [path.strip() for path in image_paths]\n",
    "\n",
    "# Copier les images dans le dossier de sortie\n",
    "for image_path in image_paths:\n",
    "    if os.path.exists(image_path):  # Vérifie que l'image existe\n",
    "        shutil.copy(image_path, output_dir)\n",
    "    else:\n",
    "        print(f\"Image non trouvée : {image_path}\")\n",
    "\n",
    "print(\"Les images ont été copiées dans le dossier 'train/images'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47f82cde-6ecb-4fe4-bef5-e06deb7e6c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tous les labels ont été copiés dans le dossier 'Train/labels'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Dossiers\n",
    "images_dir = \"/Users/zakariaeanouk/Downloads/archive (1)/Test/images\"\n",
    "labels_src_dir = \"/Users/zakariaeanouk/Downloads/archive (1)/ts/ts\"\n",
    "labels_dest_dir = \"/Users/zakariaeanouk/Downloads/archive (1)/Test/labels\"\n",
    "\n",
    "# Créer le dossier de destination des labels s'il n'existe pas\n",
    "os.makedirs(labels_dest_dir, exist_ok=True)\n",
    "\n",
    "# Parcourir toutes les images du dossier d'images\n",
    "for image_file in os.listdir(images_dir):\n",
    "    if image_file.endswith(\".jpg\"):  # Vérifiez l'extension des images\n",
    "        # Remplacer l'extension de l'image par .txt pour trouver le label correspondant\n",
    "        label_file = image_file.replace(\".jpg\", \".txt\")\n",
    "        src_label_path = os.path.join(labels_src_dir, label_file)\n",
    "        dest_label_path = os.path.join(labels_dest_dir, label_file)\n",
    "\n",
    "        # Vérifiez si le fichier de label existe\n",
    "        if os.path.exists(src_label_path):\n",
    "            shutil.copy(src_label_path, dest_label_path)  # Copier le fichier label\n",
    "        else:\n",
    "            print(f\"Label non trouvé pour l'image : {image_file}\")\n",
    "\n",
    "print(\"Tous les labels ont été copiés dans le dossier 'Train/labels'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ead9511c-4fd1-48dd-aa02-804a637f32c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le dataset a été divisé en Train et Validation avec succès.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Chemins vers les dossiers d'entrée et de sortie\n",
    "images_dir = \"//Users/zakariaeanouk/Downloads/archive (1)/Train/images\"\n",
    "labels_dir = \"/Users/zakariaeanouk/Downloads/archive (1)/Train/labels\"\n",
    "output_dir = \"/Users/zakariaeanouk/Downloads/archive (1)/\"\n",
    "\n",
    "# Dossiers de sortie pour le nouveau Train et Validation\n",
    "new_train_images_dir = os.path.join(output_dir, \"New_Train/images\")\n",
    "new_train_labels_dir = os.path.join(output_dir, \"New_Train/labels\")\n",
    "val_images_dir = os.path.join(output_dir, \"New_Train\")\n",
    "val_labels_dir = os.path.join(output_dir, \"Validation/labels\")\n",
    "\n",
    "# Créer les dossiers pour Train et Validation\n",
    "os.makedirs(new_train_images_dir, exist_ok=True)\n",
    "os.makedirs(new_train_labels_dir, exist_ok=True)\n",
    "os.makedirs(val_images_dir, exist_ok=True)\n",
    "os.makedirs(val_labels_dir, exist_ok=True)\n",
    "\n",
    "# Liste de toutes les images et leurs labels correspondants\n",
    "all_images = [f for f in os.listdir(images_dir) if f.endswith(\".jpg\")]\n",
    "all_labels = [f.replace(\".jpg\", \".txt\") for f in all_images]\n",
    "\n",
    "# Diviser les données : 80% pour Train et 20% pour Validation\n",
    "train_images, val_images = train_test_split(all_images, test_size=0.2, random_state=42)\n",
    "\n",
    "# Copier les fichiers dans les dossiers correspondants\n",
    "for image in train_images:\n",
    "    shutil.copy(os.path.join(images_dir, image), os.path.join(new_train_images_dir, image))\n",
    "    shutil.copy(os.path.join(labels_dir, image.replace(\".jpg\", \".txt\")), os.path.join(new_train_labels_dir, image.replace(\".jpg\", \".txt\")))\n",
    "\n",
    "for image in val_images:\n",
    "    shutil.copy(os.path.join(images_dir, image), os.path.join(val_images_dir, image))\n",
    "    shutil.copy(os.path.join(labels_dir, image.replace(\".jpg\", \".txt\")), os.path.join(val_labels_dir, image.replace(\".jpg\", \".txt\")))\n",
    "\n",
    "print(\"Le dataset a été divisé en Train et Validation avec succès.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b323ca47-014d-4dd6-88a3-7ec80ee7fac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion des classes terminée.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Chemin vers les fichiers de labels\n",
    "labels_dir = \"/Users/zakariaeanouk/Downloads/archive (1)/Test/labels\"\n",
    "\n",
    "# Parcourir tous les fichiers de labels\n",
    "for label_file in os.listdir(labels_dir):\n",
    "    if label_file.endswith(\".txt\"):  # Vérifie que c'est un fichier de labels\n",
    "        file_path = os.path.join(labels_dir, label_file)\n",
    "        with open(file_path, \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "        \n",
    "        # Modifier chaque ligne pour mettre la classe à 0\n",
    "        new_lines = []\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            parts[0] = \"0\"  # Remplace le class_id par 0\n",
    "            new_lines.append(\" \".join(parts))\n",
    "        \n",
    "        # Écrire les modifications dans le fichier\n",
    "        with open(file_path, \"w\") as file:\n",
    "            file.write(\"\\n\".join(new_lines))\n",
    "\n",
    "print(\"Conversion des classes terminée.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a8e2438-c20e-4c0a-926b-b7c65b6f711c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur dans le fichier YAML : mapping values are not allowed here\n",
      "  in \"/Users/zakariaeanouk/Downloads/archive (1)/data.yaml\", line 5, column 3\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "try:\n",
    "    with open('/Users/zakariaeanouk/Downloads/archive (1)/data.yaml', 'r') as file:\n",
    "        data = yaml.safe_load(file)\n",
    "    print(\"Fichier YAML chargé avec succès :\", data)\n",
    "except yaml.YAMLError as e:\n",
    "    print(\"Erreur dans le fichier YAML :\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5bdb7511-94aa-42eb-bfa1-cb4914c4ff2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m classification_model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/zakariaeanouk/Desktop/projet ML/new_best.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Charger l'image\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mload_image\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/zakariaeanouk/Downloads/archive (2)/car/test/images/road773_png.rf.b467561d9bc1abfaa69c770187f44c38.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Étape 1 : Détection\u001b[39;00m\n\u001b[1;32m      9\u001b[0m detections \u001b[38;5;241m=\u001b[39m detection_model\u001b[38;5;241m.\u001b[39mpredict(image)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_image' is not defined"
     ]
    }
   ],
   "source": [
    "# Charger les modèles\n",
    "detection_model = YOLO('/Users/zakariaeanouk/Desktop/projet ML/detection/best.pt')\n",
    "classification_model = YOLO('/Users/zakariaeanouk/Desktop/projet ML/new_best.pt')\n",
    "\n",
    "# Charger l'image\n",
    "image = load_image('/Users/zakariaeanouk/Downloads/archive (2)/car/test/images/road773_png.rf.b467561d9bc1abfaa69c770187f44c38.jpg')\n",
    "\n",
    "# Étape 1 : Détection\n",
    "detections = detection_model.predict(image)\n",
    "\n",
    "# Étape 2 : Classification\n",
    "for box in detections:\n",
    "    x1, y1, x2, y2 = box[:4]\n",
    "    roi = image[y1:y2, x1:x2]\n",
    "    class_label = classification_model.predict(roi)\n",
    "    print(f\"Panneau détecté : {class_label} à {box}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c3336e51-507b-427f-bab7-02b65fb4b75b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 1024x768 1 panneau, 231.4ms\n",
      "Speed: 16.6ms preprocess, 231.4ms inference, 1.1ms postprocess per image at shape (1, 3, 1024, 768)\n",
      "Results saved to \u001b[1mruns/detect/predict88\u001b[0m\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'panneau'}\n",
      "obb: None\n",
      "orig_img: array([[[ 59, 145, 248],\n",
      "        [ 59, 145, 248],\n",
      "        [ 59, 145, 248],\n",
      "        ...,\n",
      "        [ 91,  51,  39],\n",
      "        [ 94,  54,  42],\n",
      "        [ 97,  57,  45]],\n",
      "\n",
      "       [[ 59, 145, 248],\n",
      "        [ 59, 145, 248],\n",
      "        [ 59, 145, 248],\n",
      "        ...,\n",
      "        [ 90,  50,  38],\n",
      "        [ 90,  50,  38],\n",
      "        [ 91,  51,  39]],\n",
      "\n",
      "       [[ 59, 145, 248],\n",
      "        [ 59, 145, 248],\n",
      "        [ 59, 145, 248],\n",
      "        ...,\n",
      "        [ 88,  48,  36],\n",
      "        [ 85,  45,  33],\n",
      "        [ 85,  45,  33]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[115, 144, 188],\n",
      "        [115, 144, 188],\n",
      "        [114, 143, 187],\n",
      "        ...,\n",
      "        [ 42,  52,  77],\n",
      "        [ 45,  55,  82],\n",
      "        [ 45,  55,  82]],\n",
      "\n",
      "       [[122, 151, 195],\n",
      "        [116, 145, 189],\n",
      "        [108, 137, 181],\n",
      "        ...,\n",
      "        [ 44,  54,  81],\n",
      "        [ 50,  60,  87],\n",
      "        [ 54,  64,  91]],\n",
      "\n",
      "       [[125, 154, 198],\n",
      "        [113, 142, 186],\n",
      "        [ 98, 127, 171],\n",
      "        ...,\n",
      "        [ 45,  55,  82],\n",
      "        [ 55,  65,  92],\n",
      "        [ 63,  73, 100]]], dtype=uint8)\n",
      "orig_shape: (4032, 3024)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/detect/predict88'\n",
      "speed: {'preprocess': 16.63994789123535, 'inference': 231.43696784973145, 'postprocess': 1.09100341796875}\n",
      "Bounding boxes: [[     1692.5      1454.7      1796.2      1559.6]]\n",
      "\n",
      "0: 640x640 1 Speed_Limit_60, 320.7ms\n",
      "Speed: 0.0ms preprocess, 320.7ms inference, 7.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Panneau détecté : [ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'Speed_Limit_20', 1: 'Speed_Limit_30', 2: 'Speed_Limit_50', 3: 'Speed_Limit_60', 4: 'Speed_Limit_70', 5: 'Speed_Limit_80', 6: 'End_of_speed_limit', 7: 'Speed_Limit_100', 8: 'Speed_Limit_120', 9: 'No_overtaking', 10: 'No_overtaking_by_trucks', 11: 'Intersection_with_a_minor_road', 12: 'Priority_road', 13: 'Give_way', 14: 'Stop', 15: 'Road_closed_to_all_vehicles', 16: 'No_trucks', 17: 'No_entry', 18: 'Other_dangers', 19: 'Curve_to_the_left', 20: 'Curve_to_the_right', 21: 'Double_curve_first_to_left', 22: 'Uneven_road', 23: 'Slippery_road', 24: 'Road_narrows_on_the_right', 25: 'Roadworks', 26: 'Traffic_lights', 27: 'Pedestrian_crossing_ahead', 28: 'Children', 29: 'Bicycle crossing', 30: 'Snow', 31: 'Deer_crossing', 32: 'End_of_all_restrictions_and_prohibitions', 33: 'Turn_right_only', 34: 'Turn_left_only', 35: 'Go_straight_only', 36: 'Proceed_straight_or_turn_right', 37: 'Proceed_straight_or_turn_left', 38: 'Keep_right', 39: 'Keep_left', 40: 'Roundabout', 41: 'End_overtaking_prohibition', 42: 'End_overtaking_prohibition_for_trucks'}\n",
      "obb: None\n",
      "orig_img: array([[[129, 146, 162],\n",
      "        [129, 146, 162],\n",
      "        [129, 146, 162],\n",
      "        ...,\n",
      "        [166, 183, 211],\n",
      "        [166, 183, 211],\n",
      "        [166, 183, 211]],\n",
      "\n",
      "       [[129, 146, 162],\n",
      "        [129, 146, 162],\n",
      "        [129, 146, 162],\n",
      "        ...,\n",
      "        [166, 183, 211],\n",
      "        [166, 183, 211],\n",
      "        [166, 183, 211]],\n",
      "\n",
      "       [[129, 146, 162],\n",
      "        [129, 146, 162],\n",
      "        [129, 146, 162],\n",
      "        ...,\n",
      "        [166, 183, 211],\n",
      "        [166, 183, 211],\n",
      "        [166, 183, 211]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[124, 105, 124],\n",
      "        [124, 105, 124],\n",
      "        [124, 105, 124],\n",
      "        ...,\n",
      "        [105,  91,  91],\n",
      "        [105,  91,  91],\n",
      "        [105,  91,  91]],\n",
      "\n",
      "       [[124, 105, 124],\n",
      "        [124, 105, 124],\n",
      "        [124, 105, 124],\n",
      "        ...,\n",
      "        [105,  91,  91],\n",
      "        [105,  91,  91],\n",
      "        [105,  91,  91]],\n",
      "\n",
      "       [[124, 105, 124],\n",
      "        [124, 105, 124],\n",
      "        [124, 105, 124],\n",
      "        ...,\n",
      "        [105,  91,  91],\n",
      "        [105,  91,  91],\n",
      "        [105,  91,  91]]], dtype=uint8)\n",
      "orig_shape: (640, 640)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/detect/predict89'\n",
      "speed: {'preprocess': 0.020265579223632812, 'inference': 320.723295211792, 'postprocess': 7.676124572753906}] à la boîte [     1692.5      1454.7      1796.2      1559.6]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "from ultralytics import YOLO  # Assurez-vous d'importer YOLO si ce n'est pas déjà fait\n",
    "\n",
    "# Charger les modèles\n",
    "detection_model = YOLO('/Users/zakariaeanouk/Desktop/projet ML/detection/best.pt')\n",
    "classification_model = YOLO('/Users/zakariaeanouk/Desktop/projet ML/best.pt')\n",
    "\n",
    "# Charger l'image (utilisation de PIL)\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')  # Charger en mode RGB\n",
    "    return np.array(image)\n",
    "\n",
    "image = load_image('/Users/zakariaeanouk/Downloads/WhatsApp Image 2024-12-05 at 17.54.43.jpeg')\n",
    "\n",
    "# Étape 1 : Détection\n",
    "detections = detection_model.predict(image, save=True,imgsz=1024)  # Détection des bounding boxes\n",
    "print(detections[0])\n",
    "\n",
    "# Étape 2 : Classification\n",
    "if detections:\n",
    "    result = detections[0]  # Access the first result if there are multiple\n",
    "    boxes = result.boxes.xyxy  # Coordinates of the bounding boxes in [x_min, y_min, x_max, y_max]\n",
    "    boxes = boxes.cpu().numpy()  # Convert to numpy array (if you're using PyTorch)\n",
    "    print(\"Bounding boxes:\", boxes)\n",
    "else:\n",
    "    print(\"No detections found.\")\n",
    "\n",
    "# Traitement de chaque panneau détecté\n",
    "for box in boxes:\n",
    "    x1, y1, x2, y2 = map(int, box[:4])  # Convertir les coordonnées en entiers\n",
    "    roi = image[y1:y2, x1:x2]  # Extraire la région d'intérêt (ROI)\n",
    "    \n",
    "    # Redimensionner la ROI à la taille attendue par le modèle de classification (640, 640)\n",
    "    roi_resized = cv2.resize(roi, (640, 640))  # Resize to (640, 640) or any size divisible by 32\n",
    "    \n",
    "    # Conversion de la ROI en tensor\n",
    "    roi_tensor = torch.tensor(roi_resized).permute(2, 0, 1).unsqueeze(0).float()  # Convert to tensor\n",
    "\n",
    "    # Si les valeurs des pixels sont dans la plage 0-255, normalisez-les entre 0 et 1\n",
    "    roi_tensor = roi_tensor / 255.0  # Normalisation\n",
    "\n",
    "    # Prédiction avec le modèle de classification\n",
    "    class_label = classification_model.predict(roi_tensor)  # Prediction with the resized ROI\n",
    "    print(f\"Panneau détecté : {class_label} à la boîte {box}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "9f998452-5d8b-42c3-b5a1-f50b191c5589",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "PytorchStreamReader failed reading zip archive: failed finding central directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[179], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m classification_model \u001b[38;5;241m=\u001b[39m \u001b[43mYOLO\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/zakariaeanouk/Desktop/projet ML/yolov8n.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m result\u001b[38;5;241m=\u001b[39mclassification_model(source\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ultralytics/models/yolo/model.py:23\u001b[0m, in \u001b[0;36mYOLO.__init__\u001b[0;34m(self, model, task, verbose)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m \u001b[38;5;241m=\u001b[39m new_instance\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Continue with default YOLO initialization\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ultralytics/engine/model.py:145\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, model, task, verbose)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(model, task\u001b[38;5;241m=\u001b[39mtask, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ultralytics/engine/model.py:285\u001b[0m, in \u001b[0;36mModel._load\u001b[0;34m(self, weights, task)\u001b[0m\n\u001b[1;32m    282\u001b[0m weights \u001b[38;5;241m=\u001b[39m checks\u001b[38;5;241m.\u001b[39mcheck_model_file_from_stem(weights)  \u001b[38;5;66;03m# add suffix, i.e. yolov8n -> yolov8n.pt\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Path(weights)\u001b[38;5;241m.\u001b[39msuffix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;241m=\u001b[39m \u001b[43mattempt_load_one_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset_ckpt_args(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ultralytics/nn/tasks.py:910\u001b[0m, in \u001b[0;36mattempt_load_one_weight\u001b[0;34m(weight, device, inplace, fuse)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mattempt_load_one_weight\u001b[39m(weight, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fuse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    909\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a single model weights.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 910\u001b[0m     ckpt, weight \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_safe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load ckpt\u001b[39;00m\n\u001b[1;32m    911\u001b[0m     args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mDEFAULT_CFG_DICT, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_args\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))}  \u001b[38;5;66;03m# combine model and default args, preferring model args\u001b[39;00m\n\u001b[1;32m    912\u001b[0m     model \u001b[38;5;241m=\u001b[39m (ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mema\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ultralytics/nn/tasks.py:837\u001b[0m, in \u001b[0;36mtorch_safe_load\u001b[0;34m(weight, safe_only)\u001b[0m\n\u001b[1;32m    835\u001b[0m                 ckpt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(f, pickle_module\u001b[38;5;241m=\u001b[39msafe_pickle)\n\u001b[1;32m    836\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 837\u001b[0m             ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# e.name is missing module name\u001b[39;00m\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ultralytics/utils/patches.py:86\u001b[0m, in \u001b[0;36mtorch_load\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TORCH_1_13 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_only\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m     84\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_only\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_torch_load\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/serialization.py:1005\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1003\u001b[0m orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n\u001b[1;32m   1004\u001b[0m overall_storage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1005\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_torchscript_zip(opened_zipfile):\n\u001b[1;32m   1007\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m received a zip file that looks like a TorchScript archive\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1008\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m dispatching to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.jit.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (call \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.jit.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directly to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1009\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m silence this warning)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mUserWarning\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/serialization.py:457\u001b[0m, in \u001b[0;36m_open_zipfile_reader.__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name_or_buffer) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory"
     ]
    }
   ],
   "source": [
    "classification_model = YOLO('/Users/zakariaeanouk/Desktop/projet ML/yolov8n.pt')\n",
    "result=classification_model(source=1,show=True,save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fefe467d-9269-462d-a3ac-c68e42908b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 (no detections), 194.8ms\n",
      "Speed: 14.1ms preprocess, 194.8ms inference, 8.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict178\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 206.3ms\n",
      "Speed: 8.9ms preprocess, 206.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict179\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 145.6ms\n",
      "Speed: 4.1ms preprocess, 145.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict180\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 102.4ms\n",
      "Speed: 5.0ms preprocess, 102.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict181\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 114.2ms\n",
      "Speed: 3.7ms preprocess, 114.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict182\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 119.5ms\n",
      "Speed: 6.3ms preprocess, 119.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict183\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 91.7ms\n",
      "Speed: 5.0ms preprocess, 91.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict184\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 94.4ms\n",
      "Speed: 3.7ms preprocess, 94.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict185\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 109.5ms\n",
      "Speed: 3.9ms preprocess, 109.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict186\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 122.5ms\n",
      "Speed: 4.2ms preprocess, 122.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict187\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 88.2ms\n",
      "Speed: 3.1ms preprocess, 88.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict188\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 115.4ms\n",
      "Speed: 4.2ms preprocess, 115.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict189\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 148.9ms\n",
      "Speed: 4.8ms preprocess, 148.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict190\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 95.6ms\n",
      "Speed: 3.8ms preprocess, 95.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict191\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 145.5ms\n",
      "Speed: 9.2ms preprocess, 145.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict192\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 143.9ms\n",
      "Speed: 4.2ms preprocess, 143.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict193\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 183.3ms\n",
      "Speed: 7.7ms preprocess, 183.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict194\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 196.3ms\n",
      "Speed: 4.2ms preprocess, 196.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict195\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 113.7ms\n",
      "Speed: 4.7ms preprocess, 113.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict196\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 120.0ms\n",
      "Speed: 5.2ms preprocess, 120.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict197\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 164.9ms\n",
      "Speed: 6.7ms preprocess, 164.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict198\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 224.8ms\n",
      "Speed: 15.8ms preprocess, 224.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict199\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 153.0ms\n",
      "Speed: 7.5ms preprocess, 153.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict200\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 149.6ms\n",
      "Speed: 3.8ms preprocess, 149.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict201\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 132.1ms\n",
      "Speed: 5.0ms preprocess, 132.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict202\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 135.9ms\n",
      "Speed: 6.4ms preprocess, 135.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict203\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 199.0ms\n",
      "Speed: 5.5ms preprocess, 199.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict204\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 180.0ms\n",
      "Speed: 4.3ms preprocess, 180.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict205\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 135.7ms\n",
      "Speed: 3.9ms preprocess, 135.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict206\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 123.6ms\n",
      "Speed: 4.9ms preprocess, 123.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict207\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 125.4ms\n",
      "Speed: 3.5ms preprocess, 125.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict208\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 336.1ms\n",
      "Speed: 33.0ms preprocess, 336.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict209\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 200.1ms\n",
      "Speed: 4.7ms preprocess, 200.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict210\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 137.3ms\n",
      "Speed: 8.5ms preprocess, 137.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict211\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 147.9ms\n",
      "Speed: 4.0ms preprocess, 147.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict212\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 110.6ms\n",
      "Speed: 3.7ms preprocess, 110.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict213\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 110.6ms\n",
      "Speed: 10.5ms preprocess, 110.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict214\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 145.8ms\n",
      "Speed: 4.1ms preprocess, 145.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict215\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 139.0ms\n",
      "Speed: 4.0ms preprocess, 139.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict216\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 135.9ms\n",
      "Speed: 3.7ms preprocess, 135.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict217\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 133.3ms\n",
      "Speed: 4.3ms preprocess, 133.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict218\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 164.9ms\n",
      "Speed: 3.2ms preprocess, 164.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict219\u001b[0m\n",
      "\n",
      "0: 384x640 (no detections), 281.3ms\n",
      "Speed: 26.4ms preprocess, 281.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict220\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Détection en temps réel\u001b[39;00m\n\u001b[1;32m     12\u001b[0m classification_model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/zakariaeanouk/Desktop/projet ML/best.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mclassification_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m annotated_frame \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot()\n\u001b[1;32m     16\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraffic Sign Detection\u001b[39m\u001b[38;5;124m\"\u001b[39m, annotated_frame)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ultralytics/engine/model.py:176\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    149\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    150\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    152\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    153\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ultralytics/engine/model.py:554\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 554\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ultralytics/engine/predictor.py:173\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ultralytics/engine/predictor.py:259\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 259\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ultralytics/engine/predictor.py:143\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[0;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    138\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    139\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    142\u001b[0m )\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ultralytics/nn/autobackend.py:526\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[0;32m--> 526\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ultralytics/nn/tasks.py:112\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ultralytics/nn/tasks.py:130\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ultralytics/nn/tasks.py:151\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 151\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m    152\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ultralytics/nn/modules/block.py:238\u001b[0m, in \u001b[0;36mC2f.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[1;32m    237\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 238\u001b[0m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ultralytics/nn/modules/block.py:238\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[1;32m    237\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 238\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ultralytics/nn/modules/block.py:347\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    346\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Applies the YOLO FPN to input data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ultralytics/nn/modules/conv.py:54\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply convolution and activation without batch normalization.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO \n",
    "\n",
    "cap = cv2.VideoCapture(1)  # Webcam\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Détection en temps réel\n",
    "    classification_model = YOLO('/Users/zakariaeanouk/Desktop/projet ML/best.pt')\n",
    "    results = classification_model(frame,save=True)\n",
    "    annotated_frame = results[0].plot()\n",
    "    \n",
    "    cv2.imshow(\"Traffic Sign Detection\", annotated_frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6590617f-48a0-4405-ac90-7ec417626a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
